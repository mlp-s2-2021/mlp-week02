{"cells":[{"cell_type":"markdown","source":"# Machine Learning in Python - Workshop 2","metadata":{"cell_id":"00000-34f9557d-8f15-494e-8d65-74c8ae429c5c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 1. Setup\n\n### 1.1 Packages\nIn the cell below we will load the core libraries we will be using for this workshop and setting some sensible defaults for our plot size and resolution.","metadata":{"cell_id":"00001-645a25eb-6010-425a-88c0-ecf0093a9edc","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00002-d0af5d8f-8894-4c5a-b754-353993666790","output_cleared":true,"deepnote_cell_type":"code"},"source":"# Display plots inline\n%matplotlib inline\n\n# Data libraries\nimport pandas as pd\nimport numpy as np\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plotting defaults\nplt.rcParams['figure.figsize'] = (8,5)\nplt.rcParams['figure.dpi'] = 80","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Data\n\nTo begin, we will examine a simple data set on the size and weight of a number of books. These data come from the `allbacks` data set from the `DAAG` package in R. Our goal is to model the weight of a book using some combination of the other features in the data. The included columns are as follows:\n\n* `volume` - book volumes in cubic centimeters\n* `area` - hard board cover areas in square centimeters\n* `weight` - book weights in grams\n* `cover` - a factor with levels `\"hb\"` hardback, `\"pb\"` paperback\n\nWe read the data into python using pandas,","metadata":{"cell_id":"00003-90709695-8746-4669-9199-fd144a6ec872","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00004-a7f234aa-3b37-4fda-937b-9e437a814ec1","output_cleared":true,"deepnote_cell_type":"code"},"source":"books = pd.read_csv(\"daag_books.csv\")\nbooks","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 1\n\nCreate a pairs plot of these data (make sure to include the `cover` column), describe any relationships you observe in the data.","metadata":{"cell_id":"00005-da6d5f4b-bf6b-43f7-9751-a015e6a9924e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00006-4d9907ea-c7c7-4c16-aee7-5e12c3e00cc1","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"cell_id":"00007-a3d1594c-000b-48dd-b347-dfa5410fe0a7","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## 1. Regression","metadata":{"cell_id":"00008-7d871b29-2a08-4b7f-8421-d36b23056000","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We will begin by fitting a simple linear regression model for `weight` exclusively using `volume` as a feature in our model. ","metadata":{"cell_id":"00009-ca8096af-6bc2-4bf1-aea6-19d55952f208","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 2\n\nCreate a scatter plot of these data describe any apparent relationship between `weight` and `volume`.","metadata":{"cell_id":"00010-92e05d0b-dedd-444b-a9f3-df423c4b0e70","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00011-abd9df3c-dd0b-43bc-a4b4-4369e7a9cc3e","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"cell_id":"00012-63b4a2f0-021b-4255-871c-415d2730d90b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 1.1 Least Squares","metadata":{"cell_id":"00013-6f049218-7709-422f-9a61-d6b2fd6da8da","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"In lecture we discussed how we can represent a regression problem using matrix notation and we can derive a solution using least squares. We can express this as,\n\n$$\n\\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\,\\, \\lVert \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\rVert^2 \n= \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\,\\, (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta})^\\top(\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta})\n$$\n\nwhere,\n\n$$\n\\underset{n \\times 1}{\\boldsymbol{y}} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n \\end{pmatrix} \n\\qquad\n\\underset{n \\times 2}{\\boldsymbol{X}} = \\begin{pmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_{n-1} \\\\ 1 & x_n \\\\ \\end{pmatrix}\n\\qquad \n\\underset{2 \\times 1}{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\n$$","metadata":{"cell_id":"00014-fbc0c5b1-65c6-41e3-b85a-6000792a1342","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The solution to this optimization problem is,\n\n$$\n\\boldsymbol{\\beta} = \\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^\\top\\boldsymbol{y}\n$$","metadata":{"cell_id":"00015-e55877d4-ea16-4e4a-8b06-c452fee69c1e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"In Python we can construct the model matrix `X` by combining a column of ones, for the intercept, with our observed `volume` values. Similarly, `y` is a column vector of the `weight` values. In both cases we construct these objects as numpy array objects.","metadata":{"cell_id":"00016-0a1e2d21-4ae7-45f0-b0d5-da7f9fc56067","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00017-21e29cd3-56a0-4b48-899b-4409e4b6f9e2","output_cleared":true,"deepnote_cell_type":"code"},"source":"y = np.array(books.weight)\nprint(y)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00018-7d43b31a-6d5b-481f-9f2f-172e204c6c9a","output_cleared":true,"deepnote_cell_type":"code"},"source":"X = np.c_[\n    np.ones(len(y)),\n    books.volume\n]\n\nprint(X[:5])","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given the model matrix $(\\boldsymbol{X})$ and observed outcomes $(\\boldsymbol{y})$ we can then calculate the vector of solutions $(\\boldsymbol{\\beta})$ using numpy,","metadata":{"cell_id":"00019-17b55704-7eca-4662-a6a9-7de93e566521","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00020-f44bb993-8049-4e28-8da0-c8a2419a4950","output_cleared":true,"deepnote_cell_type":"code"},"source":"from numpy.linalg import solve\n\nbeta = solve(X.T @ X, X.T @ y)\nprint(beta)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that when using numpy `@` performs  matrix multiplication while `*` performs elementwise multiplication between arrays. Numpy matrix multiplication can also be written using `A.dot(B)` or `np.matmul(A,B)`.\n\nWe can calculate predictions from this model by calculating $\\hat{y} = \\boldsymbol{X} \\boldsymbol{\\beta}$. ","metadata":{"cell_id":"00021-611b22a9-a494-4ab7-a88a-ceed7c58b0a1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 3\n\nCalculate these predicted book weights and store them in the origin `books` data frame in a column called `weight_ls_pred`. Print out the updated version of the data frame with this new column added.","metadata":{"cell_id":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00023-04722d32-f420-4104-b25b-37656a71df76","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\nGiven the predictions we can create a plot showing the models fit by overlaying a line plot of the predictions on top of the original scatter plot.","metadata":{"cell_id":"00024-66ad5764-457d-4ffe-9f28-cd42b9a0b5b7","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00025-67f4357b-0a2c-46a2-90f1-8a75e8b62e2f","output_cleared":true,"deepnote_cell_type":"code"},"source":"sns.scatterplot(x=\"volume\", y=\"weight\", hue=\"cover\", data=books)\nsns.lineplot(x=\"volume\", y=\"weight_ls_pred\", color=\"black\", data=books)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### 1.2 scikit-Learn","metadata":{"cell_id":"00026-021075dd-c202-41db-9f49-2b4de4efd791","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Constructing the model matrix by hand and calculating $\\beta$ and model predictions using the least squares solution is less than ideal. As you might expect there are a number of higher level libraries that take care of many of these details. In this course we will be using the **scikit-learn** (**sklearn**) library to implement most of our machine learning models. As the semester progresses we will be learning about and implementing many different modeling methods. Additionally, we will also be learning how to use the larger data processing and workflow tools that are available in this library.\n\nsklearn separates its various modeling tools into submodules organized by model type - for today we will be using the `LinearRegression` model from the `linear_model` submodule. Which we can import as follows,","metadata":{"cell_id":"00027-068e26c7-da60-4c7c-8551-5f914b53af75","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00028-40915b00-d692-4d82-9df3-1a72901e798d","output_cleared":true,"deepnote_cell_type":"code"},"source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general sklearn's models are implemented by first creating a model object, which is configured via constructor arguments, and then using that object to fit your data. As such, we will now create a linear regression model object `lr` and use it to fit our data. Once this object is created we use the `fit` method to obtain a model object fitted to our data. ","metadata":{"cell_id":"00029-a3ead172-d8aa-4570-9aae-68cd6d18692d","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00030-d0c72235-001d-4cd4-a50f-302b40d9e4d3","output_cleared":true,"deepnote_cell_type":"code"},"source":"lr = LinearRegression()\nl = lr.fit(\n    # X must be a matrix so we need to reshape the column\n    X = np.array(books.volume).reshape(-1,1), \n    y = books.weight\n)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model object then has various useful methods and attributes, including `intercept_` and `coef_` which contain our estimates for $\\beta$.","metadata":{"cell_id":"00031-b1d09da5-a8c6-46db-a5ec-617662e344a9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00032-d4cd3759-a588-4d95-b34e-507416ebc046","output_cleared":true,"deepnote_cell_type":"code"},"source":"b0 = l.intercept_\nb1 = l.coef_[0]   # Subsetting here returns a scalar value\nbeta = (b0, b1)\n\nprint(beta)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using this default construction of `LinearRegression`, sklearn assumes that we have not included an intercept column (ones) in our model matrix and takes care of this for you. Additionally, since the intercept column is added the $\\beta$ estimated for this particular column is stored separately, in the `intercept_` attribue.\n\nI generally find this default behavior to be somewhat frustrating to work with, instead my preference is to handle all of the details of constructing the model matrix `X` myself and retrieving all `beta` values (including the intercept) from `coef_` directly. For example, if we use the `X` and `y` variables we defined for the least squares example above and construct the `LinearRegression` object using `fit_intercept=False` then,","metadata":{"cell_id":"00033-9aecc749-af3c-4332-a238-370b1c7febc5","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00034-f6c94c6a-6fd5-4963-86db-3a815ae09eda","output_cleared":true,"deepnote_cell_type":"code"},"source":"l = LinearRegression(fit_intercept=False).fit(X = X, y = y)\nbeta = l.coef_\n\nprint(beta)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that this is the same answers we obtained above.\n\nThe model fit objects also provide additional useful methods for evaluating the model (`score`) and calculating predictions (`predict`). Using the later we can add another column of predictions to our data frame.","metadata":{"cell_id":"00035-9bb64eec-8879-48d5-832a-101c7a3dc752","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00036-fdd889c3-0b7b-449e-a815-6e1e055f9703","output_cleared":true,"deepnote_cell_type":"code"},"source":"books[\"weight_sk_pred\"] = l.predict(X)\nbooks","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 5\n\nDo these results agree with the results we obtained when using the numpy least squares method?","metadata":{"cell_id":"00037-fcdee620-68f8-4497-80c8-de602dd5eb66","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00038-d4b68b22-3d41-40b9-94ae-16db94740467","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"cell_id":"00039-c822d4ea-c2e9-48e1-95be-ff75e367aca8","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 1.3 Residuals\n\nOne of the most useful tools for evaluating a model is to examine the residuals of that model. For any standard regression model the residual for observation $i$ is defined as $y_i - \\hat{y}_i$ where $\\hat{y}_i$ is the model's predicted value for observation $i$. As mentioned previous, for the case of linear regression\n$\\hat{y} = \\boldsymbol{X} \\boldsymbol{\\beta}$.","metadata":{"cell_id":"00040-63aa8c2c-08d1-49cc-bf68-4293361464ce","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 6\n\nCalculate the residual for each observation and store it in a column named `resid`. Using this new column create a residual plot (scatter plot of `volume` vs `resid`) for this model. Color the points based on the `cover` type of each book. ","metadata":{"cell_id":"00041-5e48aa4f-5a03-4f54-9967-6435ddd29a9a","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00042-2c8ed59a-75d5-482d-b5da-241c1e058e0f","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 7\n\nAre there any particular issues we should be concerned about with this model based on what you see in your residual plot?","metadata":{"cell_id":"00043-6f14283a-42e8-489c-ae08-16f363bb022d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00044-a439c965-a5a6-437c-9324-df0aff4a1525","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 2. Regression with Categorical Variables\n\n","metadata":{"cell_id":"00045-2510df6f-73e5-4a95-a0e6-84c3fcfc0d85","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 2.1 Dummy Coding","metadata":{"cell_id":"00046-c8a96bcf-e22c-455d-b8f7-38de9f5226d0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Based on these results, it should be clear that it is important that our model include information about whether or not a book is a hardback or paperback. As such, we need a way of encoding this information into our modeling framework. To do this we need a way of converting our string / categorical variable into a numeric representation that can be included in our model matrix.\n\nThe most common approach for doing this is called dummy coding, in the case of a binary categorical variable it involves picking one of the two levels of the categorical variable and encoding it as 1 and the other level as 0. With Python we can accomplish this by comparing our categorical vector to the value of our choice and then casting (converting) the result to an integer type.\n\nFor example if we wanted to code `hb` as 1 and `pb` as 0 we would do the following,","metadata":{"cell_id":"00047-468c75ad-fcb3-4887-a165-c6d54e30a250","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00048-e0a6938a-2785-45be-8495-613bd782b146","output_cleared":true,"deepnote_cell_type":"code"},"source":"books[\"cover_hb\"] = (books.cover == \"hb\").astype(int) # Returns either 0 or 1\nbooks","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is equivalent to using an indicator function in mathematical notation,\n\n$$ \n\\mathbb{1}_{hb_i} = \n\\begin{cases}\n1 & \\text {if cover of book $i$ is hardback} \\\\\n0 & \\text {if cover of book $i$ is paperback}\n\\end{cases}\n$$\n\nAlternatively, we can defined the opposite of this where we code `hardback` as 0 and `paperback` as 1,\n","metadata":{"cell_id":"00049-4d15a469-b618-486d-bccd-871ee7b40a5e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00050-23300ecd-113d-41d8-a559-3a5de5616782","output_cleared":true,"deepnote_cell_type":"code"},"source":"books[\"cover_pb\"] = (books.cover == \"pb\").astype(int) # Returns either 0 or 1\nbooks","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have recoded our categorical variable, `cover`, into a numerical variable we can fit a standard regression model with the form,\n\n$$ y_i = \\beta_0 + \\beta_1 \\, x_i + \\beta_2 \\, \\mathbb{1}_{hb_i} $$\n\nwhich we can represent in matrix form using, $\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta}$\nwhere $\\boldsymbol{X} = \\big[ \\boldsymbol{1},\\, \\boldsymbol{x},\\, \\boldsymbol{\\mathbb{1}_{hb}} \\big]$.\n\nUsing Python, we can use the concatenate function with our 1s column, the `volume` column, and our new dummy coded indicator column, `cover_hb`,","metadata":{"cell_id":"00051-ae64579f-1cea-4e7e-a044-10f7a2aee013","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00052-a2d2f82b-236a-4285-bb0c-c8477e3ae8df","output_cleared":true,"deepnote_cell_type":"code"},"source":"X = np.c_[np.ones(len(y)), books.volume, books.cover_hb]\nl = LinearRegression(fit_intercept=False).fit(X, books.weight)\n\nbeta = l.coef_\n\nprint(beta)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This gives us a regression equation of the form,\n\n$$ \ny_i = 13.9 + 0.72 \\, x_i + 184.0 \\, \\mathbb{1}_{hb_i} \n$$\n\nwhich can be rewritten as two separate line equations (one for each case of `cover`),\n\n$$\ny_i = \\begin{cases}\n        13.9 + 0.72 \\, x_i & \\text{if book cover $i$ is paperback} \\\\\n        (13.9 + 184.0) + 0.72 \\, x_i & \\text{if book cover $i$ is hardback} \\\\\n       \\end{cases}.\n$$\n\nWe can calculate prediction points along those lines using the following Python code in which we hard code the possible values of $\\boldsymbol{\\mathbb{1}_{hb_i}}$","metadata":{"cell_id":"00053-6b57cc0a-9f60-4df2-8bc4-7c69ac194de4","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00054-0ddac0c5-1574-49ce-9363-aac34646813b","output_cleared":true,"deepnote_cell_type":"code"},"source":"books[\"weight_hb_pred\"] = l.predict(X)\nbooks","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and we can then plot both of these lines along with the observed data.","metadata":{"cell_id":"00055-91fdbf58-e6a6-40de-b90d-0732d78cf919","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00056-93efff57-d781-4351-9b19-08e434e15f57","output_cleared":true,"deepnote_cell_type":"code"},"source":"sns.scatterplot(x=\"volume\", y=\"weight\", hue=\"cover\", data=books)\nsns.lineplot(x=\"volume\", y=\"weight_hb_pred\", hue=\"cover\", data=books)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As well as create plot a residual plot of this new model,","metadata":{"cell_id":"00057-d749fb2e-f46e-4e32-820b-0e528a5d19c5","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00058-c0a91d06-1802-44dd-b1fa-24788c4c9125","output_cleared":true,"deepnote_cell_type":"code"},"source":"books[\"resid_hb\"] = books.weight - books.weight_hb_pred\nsns.scatterplot(x=\"volume\", y=\"resid_hb\", hue=\"cover\", data=books)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 8\n\nBased on these regression fits, do you think the model including the dummy coded `cover` variable produces a \"better\" model than our first regression model which did not include `cover`? Explain. ","metadata":{"cell_id":"00059-fef18a7d-f736-4ae5-9bc0-94cceaae2b1e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00060-05c7e049-ccbf-43bb-bd35-e949d9705323","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\nNote that by including a dummy variable in our model will change the interpretation of our regression coefficients. In this context,\n\n* $\\beta_0$ - This is the expected weight of a book with a `volume` of zero and a `hardback` indicator of zero, in other words a softcover book with zero volume.\n\n* $\\beta_1$ - This is the expected additional weight a book would have if its volume were to increase by 1 cm$^3$, all else being equal.\n\n* $\\beta_2$ - This is the expected additional weight a book would have if its hardcover indicator were to increase by 1, all else being equal. However, the hardcover indicator can only be 0 or 1 and hence this is the change in weight we would expect between a softcover book and a hardcover book with the same volume. In other words, hardcover books weight 184g more than softcover books.\n\nBased on these interpretations we can see that the level that was coded as 0 (what is often called the reference level) gets folded into our intercept and the slope coefficient for the indicator provides the difference in intercept between the reference and the contrast level (level coded as 1).\n","metadata":{"cell_id":"00061-0359dd99-1936-4981-906a-149556d2e6cf","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 9\n\nRepeat the analysis above but this time fit a model using `pb` instead of `hb` in your model matrix. You should fit the new model as well as calculate the predictions for both paperback and hardback books.","metadata":{"cell_id":"00062-7215839c-f34e-4c8b-95c7-c2c9f30e3db9","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00063-ad76176a-794e-4005-ae19-0b9606a7c115","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 10\n\nWhat changed between the model with `cover_pb` vs the model with `cover_hb`? Specifically, comment on the values of $\\beta_0$, $\\beta_1$, and $\\beta_2$ and their interpretations.\n","metadata":{"cell_id":"00064-ab80c372-0087-43b2-8875-104cd0075f1d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00065-65180c5c-91ca-4811-8baa-092f0e45d275","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### 2.2 One hot encoding\n\nAnother common approach for transforming categorical variables is know as one hot encoding, in which all levels of the categorical variable are transformed into a new columns with values of 0 or 1. This is equivalent to what we have done manually above by including both `cover_hb` and `cover_pb`. This differs from dummy coding in that there is no longer a reference factor.\n\nPandas has a built-in method for performing this on categorical columns. This is easiest to see with a simple example, below we construct a data frame `df` with a single column that we transform into a one hot encoded version using panda's `get_dummies` method.","metadata":{"cell_id":"00066-fe6afd8c-708d-452c-9da5-82776f301e98","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00067-c569ae1f-50a5-48e7-b52c-9202ff2460a8","output_cleared":true,"deepnote_cell_type":"code"},"source":"df = pd.DataFrame({\"col\": [\"A\",\"B\",\"C\",\"A\", np.nan]})\ndf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00068-a8ba0c83-b1ac-49ff-b627-0acd4fca8a28","output_cleared":true,"deepnote_cell_type":"code"},"source":"pd.get_dummies(df)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also perform typical statistical dummy coding by using the `drop_first=True` argument, which excludes the first column as a reference level.","metadata":{"cell_id":"00069-60ca05bc-7266-401a-963a-14412d9d2b23","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00070-22cda6ad-2262-4f2f-bd1b-460e202aee2e","output_cleared":true,"deepnote_cell_type":"code"},"source":"pd.get_dummies(df, drop_first=True)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Missing values can also be included as an additional category via the `dummy_na=True` argument. This treats missing values as an additional category for the provided factor.","metadata":{"cell_id":"00071-6ef06938-ee0d-447e-b4f1-941961325a23","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00072-c524d8ad-4c73-4e1f-bfa9-ecabdcaca1ac","output_cleared":true,"deepnote_cell_type":"code"},"source":"pd.get_dummies(df, dummy_na=True)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now use this approach with sklearn's linear regression model to simplify the process of creating our model with both `volume` and `cover`. Using `get_dummies` on a clean copy of the `books` data frame automatically replaces the `cover` column with `cover_hb` and `cover_pb`, which should match the columns we created by hand above. ","metadata":{"cell_id":"00073-17ed189b-fa30-44f1-875a-d6a7315bfccc","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00074-45e3752f-e076-40ab-a7ea-c8a2e95cd6f6","output_cleared":true,"deepnote_cell_type":"code"},"source":"books = pd.read_csv(\"daag_books.csv\") # Reread in the data for a clean copy\nbooks = pd.get_dummies(books)\nbooks","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that `get_dummies` does not modify the underlying dataframe in place, and that it is necessary to save the result to a new variable (or overwrite the old version).","metadata":{"cell_id":"00075-57bb54b9-c865-4ca9-8da7-9a518891e2ba","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### 2.2 Least Squares & rank deficiency\n\nNow lets consider the model where we naively include both `cover_hb` and `cover_pb` as well as an intercept column in our model matrix.","metadata":{"cell_id":"00076-d00291ff-97cc-4a4a-b877-db2c8fea0050","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00077-9f26669e-cc42-448a-ae1d-ce4ee2f07591","output_cleared":true,"deepnote_cell_type":"code"},"source":"X = np.c_[\n    np.ones(len(y)), \n    books.volume, \n    books.cover_hb,\n    books.cover_pb\n]\nl = LinearRegression(fit_intercept=False).fit(X, books.weight)\n\n\nbeta = l.coef_\nprint( beta )","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 11\n\nWrite out the equations that predict weight for hardback and paperback books according to this model.  \n","metadata":{"cell_id":"00078-37df9229-80c6-4d30-8452-ad168b04e9c9","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00079-4381beac-06ee-4113-b730-7b0e0338d1a7","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 12\n\nAre the solutions ($\\beta$) given above unique? Can you find different values of $\\beta_0$, $\\beta_1$, $\\beta_2$, and $\\beta_3$ that would give you the same regression equations you wrote out in the previous exercise?","metadata":{"cell_id":"00080-588e15d7-fd66-4632-a537-a392bc86310f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"cell_id":"00081-1d932bfa-a944-4ade-a30a-809b833cb8b2","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n### &diams; Exercise 13\n\nSolve for $\\beta$ using the numpy approach mentioned in Section 1. Do the solutions differ from sklearn's solutions? Do they make sense? Explain.","metadata":{"cell_id":"00082-cc2ae745-ec4e-41bb-a558-e081eb69a9fd","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00083-24cf9f71-cacf-4424-ab28-9e053108dd38","output_cleared":true,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{"cell_id":"00084-a1333812-72e2-4f74-9b20-ef1e1437b787","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The issues we are seeing with the above approaches are occuring due to colinearity between our predictors - if you examine the data it should be clear that given any two of the intercept, `cover_hb`, and `cover_pb` it is possible to exactly determine the value of the other column. Mathematically, we describe this as these columns are linearly depenedent, which implies that our model matrix is *rank deficient*. You can check this explicitly by via the `numpy.linalg.matrix_rank` function which will report that `X` (and $\\boldsymbol{X}^\\top\\boldsymbol{X}$) are of rank 3 not 4 which is what we might have naievely expected.\n\nThis is important as the underlying linear algrebra methods used to solve for $\\beta$ for a least squares problem often implicitly assume that $\\boldsymbol{X}^\\top\\boldsymbol{X}$ is full rank in order to solve the matrix inverse and violating these assumptions can have unexpected results.","metadata":{"cell_id":"00085-db4eaf63-e5b7-4ebb-a8de-685e93159514","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---\n\n## 3. Competing the worksheet\n\nAt this point you have hopefully been able to complete all the preceeding exercises. Now \nis a good time to check the reproducibility of this document by restarting the notebook's\nkernel and rerunning all cells in order.\n\nOnce that is done and you are happy with everything, you can then run the following cell \nto generate your PDF and turn it in on gradescope under the `mlp-week02` assignment.","metadata":{"cell_id":"00086-7422c6b7-c86a-41ac-8d32-8ba6063dcba5","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"output_cleared":true,"tags":[],"cell_id":"00087-b59d2c38-cc05-4acd-b8e7-415727f07c78","deepnote_to_be_reexecuted":false,"source_hash":null,"execution_start":1611058581218,"execution_millis":15541,"deepnote_cell_type":"code"},"source":"!jupyter nbconvert --to pdf mlp-week02.ipynb","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"deepnote_notebook_id":"2f0f4e4a-50b4-476a-ac32-ea3a1e98d30c","deepnote_execution_queue":[]}}